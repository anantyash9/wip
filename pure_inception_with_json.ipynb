{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, Activation, Cropping2D, MaxPooling2D, Dropout, Reshape, \\\n",
    "    Convolution2D\n",
    "from moviepy.editor import VideoFileClip\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy72mWwAWKMK"
   },
   "source": [
    "## Visualization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7m_NY_aWKMK"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "\n",
    "_TITLE_LEFT_MARGIN = 10\n",
    "_TITLE_TOP_MARGIN = 10\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
    "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
    "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
    "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
    "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
    "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
    "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
    "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
    "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
    "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
    "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
    "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen']\n",
    "def visualize_boxes_and_labels_on_image_array(\n",
    "    image,\n",
    "    boxes,\n",
    "    classes,classes1,\n",
    "    scores,scores1,\n",
    "    category_index,category_index1,\n",
    "    instance_masks=None,\n",
    "    instance_boundaries=None,\n",
    "    keypoints=None,\n",
    "    use_normalized_coordinates=False,\n",
    "    max_boxes_to_draw=20,\n",
    "    min_score_thresh=.5,\n",
    "    agnostic_mode=False,\n",
    "    line_thickness=4,\n",
    "    groundtruth_box_visualization_color='black',\n",
    "    skip_scores=False,\n",
    "    skip_labels=False):\n",
    "  \n",
    "  # Create a display string (and color) for every box location, group any boxes\n",
    "  # that correspond to the same location.\n",
    "  box_to_display_str_map = collections.defaultdict(list)\n",
    "  box_to_color_map = collections.defaultdict(str)\n",
    "  box_to_instance_masks_map = {}\n",
    "  box_to_instance_boundaries_map = {}\n",
    "  box_to_keypoints_map = collections.defaultdict(list)\n",
    "  if not max_boxes_to_draw:\n",
    "    max_boxes_to_draw = boxes.shape[0]\n",
    "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
    "    if scores is None or scores[i] > min_score_thresh:\n",
    "      box = tuple(boxes[i].tolist())\n",
    "      if instance_masks is not None:\n",
    "        box_to_instance_masks_map[box] = instance_masks[i]\n",
    "      if instance_boundaries is not None:\n",
    "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
    "      if keypoints is not None:\n",
    "        box_to_keypoints_map[box].extend(keypoints[i])\n",
    "      if scores is None:\n",
    "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
    "      else:\n",
    "        display_str = ''\n",
    "        if not skip_labels:\n",
    "          if not agnostic_mode:\n",
    "            if classes[i] in category_index.keys():\n",
    "              class_name = category_index[classes[i]]['name']\n",
    "              class_name1 = category_index1[classes1[i]]['name']\n",
    "            else:\n",
    "              class_name = 'N/A'\n",
    "            display_str = str(class_name)\n",
    "        if not skip_scores:\n",
    "          if not display_str:\n",
    "            display_str = '{}%'.format(int(100*scores[i]))\n",
    "          else:\n",
    "            display_str = '{}: {}%'.format(class_name, int(100*scores[i]))\n",
    "            display_str +=' :: '\n",
    "            display_str += '{}: {}%'.format(class_name1, int(100*scores1[i]))\n",
    "        box_to_display_str_map[box].append(display_str)\n",
    "        \n",
    "        if agnostic_mode:\n",
    "          box_to_color_map[box] = 'DarkOrange'\n",
    "        else:\n",
    "          box_to_color_map[box] = STANDARD_COLORS[\n",
    "              classes[i] % len(STANDARD_COLORS)]\n",
    "\n",
    "  # Draw all boxes onto image.\n",
    "  for box, color in box_to_color_map.items():\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    if instance_masks is not None:\n",
    "      vis_util.draw_mask_on_image_array(\n",
    "          image,\n",
    "          box_to_instance_masks_map[box],\n",
    "          color=color\n",
    "      )\n",
    "    if instance_boundaries is not None:\n",
    "      vis_util.draw_mask_on_image_array(\n",
    "          image,\n",
    "          box_to_instance_boundaries_map[box],\n",
    "          color='red',\n",
    "          alpha=1.0\n",
    "      )\n",
    "    vis_util.draw_bounding_box_on_image_array(\n",
    "        image,\n",
    "        ymin,\n",
    "        xmin,\n",
    "        ymax,\n",
    "        xmax,\n",
    "        color=color,\n",
    "        thickness=line_thickness,\n",
    "        display_str_list=box_to_display_str_map[box],\n",
    "        use_normalized_coordinates=use_normalized_coordinates)\n",
    "    if keypoints is not None:\n",
    "      vis_util.draw_keypoints_on_image_array(\n",
    "          image,\n",
    "          box_to_keypoints_map[box],\n",
    "          color=color,\n",
    "          radius=line_thickness / 2,\n",
    "          use_normalized_coordinates=use_normalized_coordinates)\n",
    "\n",
    "  return image\n",
    "\n",
    "def make_frame_log(\n",
    "    boxes,\n",
    "    classes,classes1,\n",
    "    category_index,category_index1):\n",
    "  fishermen ={}\n",
    "  for i in range(boxes.shape[0]):\n",
    "    class_name = category_index[classes[i]]['name']\n",
    "    class_name1 = category_index1[classes1[i]]['name']\n",
    "    fishermen[i] =[class_name,class_name1]\n",
    "  return fishermen  \n",
    "\n",
    "def make_dummer_log(dict_log):\n",
    "  dumb_dict = {}\n",
    "  dumb_dict['people']=[]\n",
    "  dumb_dict['length']=[]\n",
    "  dumb_dict['fishing']=[]\n",
    "  dumb_dict['not_fishing']=[]\n",
    "  dumb_dict['geared']=[]\n",
    "  dumb_dict['not_geared']=[]\n",
    "  dumb_dict['fishing_and_not_geared']=[]\n",
    "  dumb_dict['fishing_and_geared']=[]\n",
    "  dumb_dict['not_fishing_and_geared']=[]\n",
    "  dumb_dict['not_fishing_and_not_geared']=[]\n",
    "  for i in range(1,len(dict_log)+1):\n",
    "    dumb_dict['people'].append(len(dict_log[i]))\n",
    "    people = 0\n",
    "    fishing =0\n",
    "    not_fishing =0\n",
    "    geared =0\n",
    "    not_geared =0\n",
    "    fishing_and_not_geared=0\n",
    "    fishing_and_geared=0\n",
    "    not_fishing_and_geared=0\n",
    "    not_fishing_and_not_geared=0\n",
    "    this_frame = dict_log[i]\n",
    "    for j in range(len(this_frame)):\n",
    "      if 'Fishing' in this_frame[j]:\n",
    "        fishing+=1\n",
    "        if 'Not Geared' in this_frame[j]:\n",
    "          fishing_and_not_geared+=1\n",
    "          not_geared+=1\n",
    "        else:\n",
    "          fishing_and_geared+=1\n",
    "          geared+=1\n",
    "      else:\n",
    "        not_fishing+=1\n",
    "        if 'Not Geared' in this_frame[j]:\n",
    "          not_fishing_and_not_geared+=1\n",
    "          not_geared+=1\n",
    "        else:\n",
    "          not_fishing_and_geared+=1\n",
    "          geared+=1\n",
    "    dumb_dict['fishing'].append(fishing)\n",
    "    dumb_dict['not_fishing'].append(not_fishing)\n",
    "    dumb_dict['geared'].append(geared)\n",
    "    dumb_dict['not_geared'].append(not_geared)\n",
    "    dumb_dict['fishing_and_not_geared'].append(fishing_and_not_geared)\n",
    "    dumb_dict['fishing_and_geared'].append(fishing_and_geared)\n",
    "    dumb_dict['not_fishing_and_geared'].append(not_fishing_and_geared)\n",
    "    dumb_dict['not_fishing_and_not_geared'].append(not_fishing_and_not_geared)\n",
    "  return dumb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bm0_uNRnWKMN"
   },
   "outputs": [],
   "source": [
    "class FishingDetection:\n",
    "    \n",
    "    frame = 0\n",
    "    jlog={}\n",
    "    def __init__(self, human_detection_graph_path,\n",
    "                 human_detection_graph_label_path,\n",
    "                 num_classes,\n",
    "                 fishing_classification_graph_path,\n",
    "                 fishing_classification_graph_label_path,\n",
    "                 geared_classification_graph_path):\n",
    "        self.__human_detection_graph_path = human_detection_graph_path\n",
    "        self.__human_detection_graph = FishingDetection.load_graph(self.__human_detection_graph_path)\n",
    "        self.__num_classes = num_classes\n",
    "        self.__fishing_classification_graph_path = fishing_classification_graph_path\n",
    "        self.__fishing_classification_graph = FishingDetection.load_inception(self.__fishing_classification_graph_path)\n",
    "        self.__label_map = label_map_util.load_labelmap(human_detection_graph_label_path)\n",
    "        self.__categories = label_map_util.convert_label_map_to_categories(self.__label_map,\n",
    "                                                                           max_num_classes=self.__num_classes,\n",
    "                                                                           use_display_name=True)\n",
    "        self.__category_index = label_map_util.create_category_index(self.__categories)\n",
    "\n",
    "        self.__geared_classification_graph_path = geared_classification_graph_path\n",
    "        self.__geared_classification_graph = FishingDetection.load_inception(self.__geared_classification_graph_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def nvidia_net(drop_prob = 0.2):\n",
    "      #create a sequential Model\n",
    "      model = Sequential()\n",
    "\n",
    "      #Add a Cropping layer to trim the unneeded portions of the IMAGE from the feed\n",
    "      #model.add(Cropping2D)\n",
    "      #model.add(Reshape((50,50,3), input_shape=(None,None,3))\n",
    "      model.add(Cropping2D(cropping = ((0, 0), (0,0)), input_shape = (100 ,100 ,3)))\n",
    "\n",
    "      #Normalization Layer\n",
    "      #model.add(Lambda(lambda X_input: (X_input/255.0 - 0.5)))\n",
    "\n",
    "      #Conv2D Layer 1 with 5 x 5 kernal size\n",
    "      #model.add(Convolution2D(nb_filter = 3, nb_row = 5, nb_col = 5))\n",
    "      model.add(Convolution2D(nb_filter = 12, nb_row = 3, nb_col = 3, subsample=(1,1)))\n",
    "      model.add(Activation('relu'))\n",
    "\n",
    "      #Dropout layer\n",
    "      model.add(Dropout(drop_prob))\n",
    "\n",
    "      #Conv2D Layer 2 with 5 x 5 kernal size\n",
    "      #model.add(Convolution2D(nb_filter = 24, nb_row = 5, nb_col = 5))\n",
    "      model.add(Conv2D(nb_filter = 24, nb_row = 3, nb_col = 3,subsample=(2,2)))\n",
    "      model.add(Activation('relu'))\n",
    "\n",
    "      #Conv2D Layer 3 with 5 x 5 kernal size\n",
    "      #model.add(Convolution2D(nb_filter = 36, nb_row = 5, nb_col =  5))\n",
    "      model.add(Conv2D(nb_filter = 36, nb_row = 3, nb_col =  3,subsample=(1,1)))\n",
    "      model.add(Activation('relu'))\n",
    "\n",
    "      #Dropout layer\n",
    "      model.add(Dropout(drop_prob))\n",
    "\n",
    "      #Conv2D Layer 4 with 3 x 3 kernal size\n",
    "      #model.add(Convolution2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
    "      model.add(Conv2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
    "      model.add(Activation('relu'))\n",
    "\n",
    "      #Conv2D Layer 5 with 3 x 3 kernal size\n",
    "      #model.add(Convolution2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
    "      model.add(Conv2D(nb_filter = 64, nb_row = 3, nb_col = 3))\n",
    "      model.add(Activation('relu'))\n",
    "\n",
    "      #flatten layer\n",
    "      #flatten the output from convolution Layer\n",
    "      model.add(Flatten())\n",
    "\n",
    "      #Fully connected layer 1\n",
    "      model.add(Dense(output_dim = 50))\n",
    "\n",
    "      #Fully connected Layer 2\n",
    "      model.add(Dense(output_dim = 25))\n",
    "\n",
    "      #Fully connected Layer 3\n",
    "      model.add(Dense(output_dim = 10))\n",
    "\n",
    "      #output Layers\n",
    "      model.add(Dense(output_dim = 2, activation='softmax') )\n",
    "\n",
    "      return model\n",
    "\n",
    "    @staticmethod\n",
    "    def load_graph(graph_path):\n",
    "        detection_graph = tf.Graph()\n",
    "        with detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(graph_path, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "        return detection_graph\n",
    "\n",
    "    @staticmethod\n",
    "    def load_inception(graph_path):\n",
    "        graph = tf.Graph()\n",
    "        graph_def = tf.GraphDef()\n",
    "        with open(graph_path, \"rb\") as f:\n",
    "            graph_def.ParseFromString(f.read())\n",
    "        with graph.as_default():\n",
    "            tf.import_graph_def(graph_def)\n",
    "        return graph\n",
    "\n",
    "    def load_image_into_numpy_array(self, image):\n",
    "        (im_width, im_height) = image.size\n",
    "        return np.array(image.getdata()).reshape(\n",
    "            (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "    def run_inference_for_single_image(self,\n",
    "                                       image,\n",
    "                                       graph):\n",
    "        with graph.as_default():\n",
    "            with tf.Session() as sess:\n",
    "                # Get handles to input and output tensors\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "                tensor_dict = {}\n",
    "                for key in [\n",
    "                    'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                    'detection_classes', 'detection_masks'\n",
    "                ]:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                            tensor_name)\n",
    "                if 'detection_masks' in tensor_dict:\n",
    "                    # The following processing is only for single image\n",
    "                    detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                    detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                    real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                    detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                    detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                    detection_masks_reframed = tf.cast(\n",
    "                        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                    # Follow the convention by adding back the batch dimension\n",
    "                    tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                        detection_masks_reframed, 0)\n",
    "                image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "                # Run inference\n",
    "                output_dict = sess.run(tensor_dict,\n",
    "                                       feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "                # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "                output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "                output_dict['detection_classes'] = output_dict[\n",
    "                    'detection_classes'][0].astype(np.uint8)\n",
    "                output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "                output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "                if 'detection_masks' in output_dict:\n",
    "                    output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "        return output_dict\n",
    "\n",
    "    def list_files(self, path):\n",
    "        # files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        # return files\n",
    "        return glob.glob(path)\n",
    "\n",
    "    def process_files(self,\n",
    "                      list_files,\n",
    "                      label):\n",
    "        output = []\n",
    "        for i in list_files:\n",
    "            temp = []\n",
    "            temp.append(i)\n",
    "            ground_truth = [0, 0]\n",
    "            ground_truth[label] = 1.\n",
    "            temp.append(ground_truth)\n",
    "            output.append(temp)\n",
    "        return output\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.make_square(img)\n",
    "        img = img.resize((100, 100), Image.ANTIALIAS)\n",
    "        return np.asarray(img)\n",
    "\n",
    "    def pred_keras(self,\n",
    "                   img):\n",
    "        # print(img)\n",
    "        img = self.make_square(img)\n",
    "        img = img.resize((100, 100), Image.ANTIALIAS)\n",
    "        img = np.asarray(img)\n",
    "        img = (img/255)-0.5\n",
    "        return self.__gear_model.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "    def make_square(self,\n",
    "                    im,\n",
    "                    min_size=100,\n",
    "                    fill_color=(0, 0, 0, 0)):\n",
    "        im = Image.fromarray(im, 'RGB')\n",
    "        x, y = im.size\n",
    "        size = max(min_size, x, y)\n",
    "        new_im = Image.new('RGB', (size, size), fill_color)\n",
    "        # print((int((size - x) / 2), int(size - y) / 2))\n",
    "        new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "        return new_im\n",
    "\n",
    "    @staticmethod\n",
    "    def load_trained_model_keras(weights_path):\n",
    "        model = FishingDetection.nvidia_net()\n",
    "        model.load_weights(weights_path)\n",
    "        return model\n",
    "\n",
    "    def load_labels(self, label_file):\n",
    "        label = []\n",
    "        proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
    "        for l in proto_as_ascii_lines:\n",
    "            label.append(l.rstrip())\n",
    "        return label\n",
    "\n",
    "    def read_tensor_from_image_file(self,\n",
    "                                    image_np,\n",
    "                                    input_height=299,\n",
    "                                    input_width=299,\n",
    "                                    input_mean=0,\n",
    "                                    input_std=255):\n",
    "        input_name = \"file_reader\"\n",
    "        output_name = \"normalized\"\n",
    "        file_reader = image_np\n",
    "        image_reader = tf.convert_to_tensor(image_np, np.uint8)\n",
    "        float_caster = tf.cast(image_reader, tf.float32)\n",
    "        dims_expander = tf.expand_dims(float_caster, 0)\n",
    "        resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
    "        normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
    "        sess = tf.Session()\n",
    "        result = sess.run(normalized)\n",
    "        return result\n",
    "\n",
    "    def pred_tensorflow(self,\n",
    "                        image_np,\n",
    "                        graph,\n",
    "                        input_height=299,\n",
    "                        input_width=299,\n",
    "                        input_mean=0,\n",
    "                        input_std=255):\n",
    "        sess = tf.Session(graph=graph)\n",
    "        # label_file = r'D:\\fishing_detection\\data\\output_labels.txt'\n",
    "        t = self.read_tensor_from_image_file(\n",
    "            image_np,\n",
    "            input_height=input_height,\n",
    "            input_width=input_width,\n",
    "            input_mean=input_mean,\n",
    "            input_std=input_std)\n",
    "        input_name = \"import/\" + \"Placeholder\"\n",
    "        output_name = \"import/\" + \"final_result\"\n",
    "        input_operation = graph.get_operation_by_name(input_name)\n",
    "        output_operation = graph.get_operation_by_name(output_name)\n",
    "        with sess as sess:\n",
    "            results = sess.run(output_operation.outputs[0], {\n",
    "                input_operation.outputs[0]: t\n",
    "            })\n",
    "        results = np.squeeze(results)\n",
    "        return results\n",
    "\n",
    "    def predict_fishing(self, image, boxes, classes, scores):\n",
    "        im_width, im_height = image.size\n",
    "        image_np = self.load_image_into_numpy_array(image)\n",
    "        predictions = {}\n",
    "        predictions['bo'] = []\n",
    "        predictions['res'] = []\n",
    "        predictions['score'] = []\n",
    "        predictions1 = {}\n",
    "        predictions1['bo'] = []\n",
    "        predictions1['res'] = []\n",
    "        predictions1['score'] = []\n",
    "        for i in range(len(boxes)):\n",
    "            if classes[i] == 1 and scores[i] > 0.3:\n",
    "                predictions['bo'].append(boxes[i])\n",
    "                predictions1['bo'].append(boxes[i])\n",
    "                box = tuple(boxes[i].tolist())\n",
    "                ymin, xmin, ymax, xmax = box\n",
    "                ymin = int(ymin * im_height)\n",
    "                xmin = int(xmin * im_width)\n",
    "                ymax = int(ymax * im_height)\n",
    "                xmax = int(xmax * im_width)\n",
    "                roi = image_np[ymin:ymax, xmin:xmax]\n",
    "                # c = pred(cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\n",
    "                c = self.pred_tensorflow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB), self.__fishing_classification_graph)\n",
    "                d = self.pred_tensorflow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB), self.__geared_classification_graph)\n",
    "                if (c[0] > 0.5):\n",
    "                    predictions['res'].append(0)\n",
    "                    predictions['score'].append(c[0])\n",
    "                else:\n",
    "                    predictions['res'].append(1)\n",
    "                    predictions['score'].append(c[1])\n",
    "                if (d[0] > 0.5):\n",
    "                    predictions1['res'].append(0)\n",
    "                    predictions1['score'].append(d[0])\n",
    "                else:\n",
    "                    predictions1['res'].append(1)\n",
    "                    predictions1['score'].append(d[1])    \n",
    "        return predictions,predictions1\n",
    "\n",
    "    def pipeline(self, img):\n",
    "        FishingDetection.frame +=1\n",
    "        image = Image.fromarray(img)\n",
    "        image_np = self.load_image_into_numpy_array(image)\n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "        output_dict = self.run_inference_for_single_image(image_np, self.__human_detection_graph)\n",
    "        for i in range(len(output_dict['detection_classes'])):\n",
    "            if output_dict['detection_classes'][i] != 1:\n",
    "                output_dict['detection_scores'][i] = 0.0\n",
    "        predi,predi1 = self.predict_fishing(image, output_dict['detection_boxes'], output_dict['detection_classes'],\n",
    "                                     output_dict['detection_scores'])\n",
    "        category_ind1 = {0: {'id': 0, 'name': 'Geared'}, 1: {'id': 1, 'name': 'Not Geared'}}\n",
    "        category_ind = {1: {'id': 1, 'name': 'Not Fishing'}, 0: {'id': 0, 'name': 'Fishing'}}\n",
    "        \n",
    "        frame_log = make_frame_log(np.asarray(predi['bo'], dtype=np.float32),\n",
    "            predi['res'],predi1['res'],\n",
    "            category_ind,category_ind1,)\n",
    "        \n",
    "        FishingDetection.jlog[FishingDetection.frame]=frame_log\n",
    "        \n",
    "        visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.asarray(predi['bo'], dtype=np.float32),\n",
    "            predi['res'],predi1['res'],\n",
    "            predi['score'],predi1['score'],\n",
    "            category_ind,category_ind1,\n",
    "            instance_masks=output_dict.get('detection_masks'),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=4)\n",
    "        return image_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdyc3gIt6Xze"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    INPUT_DIRECTORY = 'input_video'\n",
    "    OUTPUT_DIRECTORY = 'output_video'\n",
    "    INPUT_FILE = 'Catching_tuna_Maldivian_style.mp4'\n",
    "    OUTPUT_FILE = 'result4.mp4'\n",
    "    vid_output = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILE)\n",
    "    vid_input = os.path.join(INPUT_DIRECTORY, INPUT_FILE)\n",
    "    subclip_start = '00:00:30.00'\n",
    "    subclip_end = '00:00:40.00'\n",
    "    clip = VideoFileClip(vid_input).subclip(subclip_start, subclip_end)\n",
    "\n",
    "    DIRECTORY_NAME = 'models'\n",
    "    MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "    TAR_EXTENSION = '.tar.gz'\n",
    "    MODEL_FILE = MODEL_NAME + TAR_EXTENSION\n",
    "    GRAPH_NAME = 'frozen_inference_graph.pb'\n",
    "    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "    PATH_TO_FROZEN_GRAPH = os.path.join(DIRECTORY_NAME, MODEL_NAME, GRAPH_NAME)\n",
    "\n",
    "    # List of the strings that is used to add correct label for each box.\n",
    "    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "    NUM_CLASSES = 90\n",
    "    fishing_detection_graph = os.path.join('colab', 'fishing_model', 'output_graph.pb')\n",
    "    geared_detection_graph = os.path.join('colab', 'gear_detect_models', 'output_graph.pb')\n",
    "    fishing_classification_graph_label_path = os.path.join('data', 'output_labels.txt')\n",
    "    object_detect = FishingDetection(human_detection_graph_path=PATH_TO_FROZEN_GRAPH,\n",
    "                                     human_detection_graph_label_path=PATH_TO_LABELS,\n",
    "                                     num_classes=NUM_CLASSES,\n",
    "                                     fishing_classification_graph_path=fishing_detection_graph,\n",
    "                                     fishing_classification_graph_label_path=fishing_classification_graph_label_path,\n",
    "                                     geared_classification_graph_path=geared_detection_graph)\n",
    "    vid = clip.fl_image(object_detect.pipeline)\n",
    "    vid.write_videofile(vid_output, audio=False, threads=4)\n",
    "    with open('data.json', 'w') as fp:\n",
    "      json.dump(FishingDetection.jlog, fp)\n",
    "    new_log = make_dummer_log(FishingDetection.jlog)\n",
    "    new_log['length'] = vid.duration\n",
    "    with open('data_disp.json', 'w') as fp:\n",
    "      json.dump(new_log, fp)\n",
    "    \n",
    "    #Reset class variables to default values  \n",
    "    FishingDetection.jlog={}\n",
    "    FishingDetection.frame=0\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3znRPqDz6j2k",
    "outputId": "b2d2cb7f-a062-4260-ddb8-f10fef28815c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_video/result4.mp4\n",
      "[MoviePy] Writing video output_video/result4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/101 [00:04<07:20,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/101 [00:08<07:15,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/101 [00:13<07:10,  4.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/101 [00:17<07:06,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 5/101 [00:22<07:03,  4.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 6/101 [00:26<07:00,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 7/101 [00:31<06:56,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 8/101 [00:35<06:52,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 9/101 [00:39<06:47,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 10/101 [00:44<06:43,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 11/101 [00:48<06:39,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 12/101 [00:53<06:34,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 13/101 [00:57<06:30,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 14/101 [01:03<06:36,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 15/101 [01:08<06:30,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 16/101 [01:12<06:24,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 17/101 [01:16<06:19,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 18/101 [01:21<06:14,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 19/101 [01:25<06:09,  4.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 20/101 [01:29<06:04,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 21/101 [01:34<05:59,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 22/101 [01:38<05:53,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 23/101 [01:42<05:48,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 24/101 [01:47<05:43,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 25/101 [01:51<05:39,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 26/101 [01:55<05:34,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 27/101 [02:00<05:29,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 28/101 [02:04<05:24,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 29/101 [02:08<05:20,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 30/101 [02:13<05:15,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 31/101 [02:17<05:11,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 32/101 [02:22<05:06,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 33/101 [02:26<05:01,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 34/101 [02:31<04:57,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 35/101 [02:35<04:53,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 36/101 [02:39<04:48,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 37/101 [02:44<04:44,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 38/101 [02:48<04:39,  4.44s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-46d733f35fc3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                                      geared_classification_graph_path=geared_detection_graph)\n\u001b[1;32m     33\u001b[0m     \u001b[0mvid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_detect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mvid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFishingDetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-175>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-174>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                            progress_bar=progress_bar)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         for t,frame in clip.iter_frames(progress_bar=progress_bar, with_times=True,\n\u001b[0;32m--> 218\u001b[0;31m                                         fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-134>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61640784182b>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detection_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         predi,predi1 = self.predict_fishing(image, output_dict['detection_boxes'], output_dict['detection_classes'],\n\u001b[0;32m--> 313\u001b[0;31m                                      output_dict['detection_scores'])\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mcategory_ind1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Geared'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Not Geared'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mcategory_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Not Fishing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Fishing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61640784182b>\u001b[0m in \u001b[0;36mpredict_fishing\u001b[0;34m(self, image, boxes, classes, scores)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;31m# c = pred(cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fishing_classification_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__geared_classification_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'res'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-61640784182b>\u001b[0m in \u001b[0;36mpred_tensorflow\u001b[0;34m(self, image_np, graph, input_height, input_width, input_mean, input_std)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             results = sess.run(output_operation.outputs[0], {\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0minput_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             })\n\u001b[1;32m    260\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
