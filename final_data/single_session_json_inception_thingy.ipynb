{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kFSqkTCdWKMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "4OkrKWRWyMlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "6b90dc06-abb4-4189-f909-4290a6266971"
      },
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting moviepy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/af/98b68b047c47d9430cb4c9ac899cf9d969de3936f888072991ea74da93a8/moviepy-0.2.3.5.tar.gz (372kB)\n",
            "\u001b[K    100% |████████████████████████████████| 378kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy) (4.3.0)\n",
            "Collecting imageio<3.0,>=2.1.2 (from moviepy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/1d/33c8686072148b3b0fcc12a2e0857dd8316b8ae20a0fa66c8d6a6d01c05c/imageio-2.3.0-py2.py3-none-any.whl (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 5.7MB/s \n",
            "\u001b[?25hCollecting tqdm<5.0,>=4.11.2 (from moviepy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from moviepy) (1.14.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio<3.0,>=2.1.2->moviepy) (0.45.1)\n",
            "Building wheels for collected packages: moviepy\n",
            "  Running setup.py bdist_wheel for moviepy ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ad/92/4d/a6c6307d4c2219d002646bd4a5987e31fd5697f6ea7778b2c0\n",
            "Successfully built moviepy\n",
            "Installing collected packages: imageio, tqdm, moviepy\n",
            "Successfully installed imageio-2.3.0 moviepy-0.2.3.5 tqdm-4.25.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vhBOxiwZpS8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c951e798-1a88-43b7-cf29-6e5641c7fe2d"
      },
      "cell_type": "code",
      "source": [
        "!git clone http://github.com/anantyash9/wip.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wip'...\n",
            "warning: redirecting to https://github.com/anantyash9/wip.git/\n",
            "remote: Counting objects: 239, done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 239 (delta 11), reused 4 (delta 1), pack-reused 217\u001b[K\n",
            "Receiving objects: 100% (239/239), 304.85 MiB | 39.04 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n",
            "Checking out files: 100% (248/248), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLTCGWyFpeS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e445af11-551f-4c53-9d50-89686bab15ed"
      },
      "cell_type": "code",
      "source": [
        "cd wip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/wip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IXjQ7YorpsAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a56e6484-8a79-47f9-d738-a242dbdffde5"
      },
      "cell_type": "code",
      "source": [
        "!git clone http://github.com/anantyash9/colab.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab'...\n",
            "warning: redirecting to https://github.com/anantyash9/colab.git/\n",
            "remote: Counting objects: 12860, done.\u001b[K\n",
            "remote: Total 12860 (delta 0), reused 0 (delta 0), pack-reused 12860\u001b[K\n",
            "Receiving objects: 100% (12860/12860), 379.25 MiB | 36.19 MiB/s, done.\n",
            "Resolving deltas: 100% (423/423), done.\n",
            "Checking out files: 100% (27631/27631), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hV4P5gyTWKMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9861ac69-2630-40b7-ca5f-cd967f26c0d2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Lambda, Conv2D, Activation, Cropping2D, MaxPooling2D, Dropout, Reshape, \\\n",
        "    Convolution2D\n",
        "from moviepy.editor import VideoFileClip\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from utils import label_map_util\n",
        "from utils import visualization_utils as vis_util\n",
        "import json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b942080/45929032 bytes (2.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2596864/45929032 bytes (5.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4644864/45929032 bytes (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8044544/45929032 bytes (17.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11526144/45929032 bytes (25.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15106048/45929032 bytes (32.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18636800/45929032 bytes (40.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22183936/45929032 bytes (48.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25731072/45929032 bytes (56.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29237248/45929032 bytes (63.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32694272/45929032 bytes (71.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35946496/45929032 bytes (78.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38813696/45929032 bytes (84.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42303488/45929032 bytes (92.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45424640/45929032 bytes (98.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wy72mWwAWKMK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualization utils"
      ]
    },
    {
      "metadata": {
        "id": "v7m_NY_aWKMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import functools\n",
        "\n",
        "_TITLE_LEFT_MARGIN = 10\n",
        "_TITLE_TOP_MARGIN = 10\n",
        "STANDARD_COLORS = [\n",
        "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n",
        "    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
        "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n",
        "    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
        "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n",
        "    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
        "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n",
        "    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
        "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n",
        "    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
        "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n",
        "    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
        "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n",
        "    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
        "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n",
        "    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
        "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n",
        "    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
        "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n",
        "    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
        "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n",
        "    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n",
        "    'WhiteSmoke', 'Yellow', 'YellowGreen']\n",
        "def visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    boxes,\n",
        "    classes,classes1,\n",
        "    scores,scores1,\n",
        "    category_index,category_index1,\n",
        "    instance_masks=None,\n",
        "    instance_boundaries=None,\n",
        "    keypoints=None,\n",
        "    use_normalized_coordinates=False,\n",
        "    max_boxes_to_draw=20,\n",
        "    min_score_thresh=.5,\n",
        "    agnostic_mode=False,\n",
        "    line_thickness=4,\n",
        "    groundtruth_box_visualization_color='black',\n",
        "    skip_scores=False,\n",
        "    skip_labels=False):\n",
        "  \n",
        "  # Create a display string (and color) for every box location, group any boxes\n",
        "  # that correspond to the same location.\n",
        "  box_to_display_str_map = collections.defaultdict(list)\n",
        "  box_to_color_map = collections.defaultdict(str)\n",
        "  box_to_instance_masks_map = {}\n",
        "  box_to_instance_boundaries_map = {}\n",
        "  box_to_keypoints_map = collections.defaultdict(list)\n",
        "  if not max_boxes_to_draw:\n",
        "    max_boxes_to_draw = boxes.shape[0]\n",
        "  for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
        "    if scores is None or scores[i] > min_score_thresh:\n",
        "      box = tuple(boxes[i].tolist())\n",
        "      if instance_masks is not None:\n",
        "        box_to_instance_masks_map[box] = instance_masks[i]\n",
        "      if instance_boundaries is not None:\n",
        "        box_to_instance_boundaries_map[box] = instance_boundaries[i]\n",
        "      if keypoints is not None:\n",
        "        box_to_keypoints_map[box].extend(keypoints[i])\n",
        "      if scores is None:\n",
        "        box_to_color_map[box] = groundtruth_box_visualization_color\n",
        "      else:\n",
        "        display_str = ''\n",
        "        if not skip_labels:\n",
        "          if not agnostic_mode:\n",
        "            if classes[i] in category_index.keys():\n",
        "              class_name = category_index[classes[i]]['name']\n",
        "              class_name1 = category_index1[classes1[i]]['name']\n",
        "            else:\n",
        "              class_name = 'N/A'\n",
        "            display_str = str(class_name)\n",
        "        if not skip_scores:\n",
        "          if not display_str:\n",
        "            display_str = '{}%'.format(int(100*scores[i]))\n",
        "          else:\n",
        "            display_str = '{}: {}%'.format(class_name, int(100*scores[i]))\n",
        "            display_str +=' :: '\n",
        "            display_str += '{}: {}%'.format(class_name1, int(100*scores1[i]))\n",
        "        box_to_display_str_map[box].append(display_str)\n",
        "        \n",
        "        if agnostic_mode:\n",
        "          box_to_color_map[box] = 'DarkOrange'\n",
        "        else:\n",
        "          box_to_color_map[box] = STANDARD_COLORS[\n",
        "              classes[i] % len(STANDARD_COLORS)]\n",
        "\n",
        "  # Draw all boxes onto image.\n",
        "  for box, color in box_to_color_map.items():\n",
        "    ymin, xmin, ymax, xmax = box\n",
        "    if instance_masks is not None:\n",
        "      vis_util.draw_mask_on_image_array(\n",
        "          image,\n",
        "          box_to_instance_masks_map[box],\n",
        "          color=color\n",
        "      )\n",
        "    if instance_boundaries is not None:\n",
        "      vis_util.draw_mask_on_image_array(\n",
        "          image,\n",
        "          box_to_instance_boundaries_map[box],\n",
        "          color='red',\n",
        "          alpha=1.0\n",
        "      )\n",
        "    vis_util.draw_bounding_box_on_image_array(\n",
        "        image,\n",
        "        ymin,\n",
        "        xmin,\n",
        "        ymax,\n",
        "        xmax,\n",
        "        color=color,\n",
        "        thickness=line_thickness,\n",
        "        display_str_list=box_to_display_str_map[box],\n",
        "        use_normalized_coordinates=use_normalized_coordinates)\n",
        "    if keypoints is not None:\n",
        "      vis_util.draw_keypoints_on_image_array(\n",
        "          image,\n",
        "          box_to_keypoints_map[box],\n",
        "          color=color,\n",
        "          radius=line_thickness / 2,\n",
        "          use_normalized_coordinates=use_normalized_coordinates)\n",
        "\n",
        "  return image\n",
        "\n",
        "def make_frame_log(\n",
        "    boxes,\n",
        "    classes,classes1,\n",
        "    category_index,category_index1):\n",
        "  fishermen ={}\n",
        "  for i in range(boxes.shape[0]):\n",
        "    class_name = category_index[classes[i]]['name']\n",
        "    class_name1 = category_index1[classes1[i]]['name']\n",
        "    fishermen[i] =[class_name,class_name1]\n",
        "  return fishermen  \n",
        "\n",
        "def make_dummer_log(dict_log):\n",
        "  dumb_dict = {}\n",
        "  dumb_dict['people']=[]\n",
        "  dumb_dict['length']=[]\n",
        "  dumb_dict['fishing']=[]\n",
        "  dumb_dict['not_fishing']=[]\n",
        "  dumb_dict['geared']=[]\n",
        "  dumb_dict['not_geared']=[]\n",
        "  dumb_dict['fishing_and_not_geared']=[]\n",
        "  dumb_dict['fishing_and_geared']=[]\n",
        "  dumb_dict['not_fishing_and_geared']=[]\n",
        "  dumb_dict['not_fishing_and_not_geared']=[]\n",
        "  for i in range(1,len(dict_log)+1):\n",
        "    dumb_dict['people'].append(len(dict_log[i]))\n",
        "    people = 0\n",
        "    fishing =0\n",
        "    not_fishing =0\n",
        "    geared =0\n",
        "    not_geared =0\n",
        "    fishing_and_not_geared=0\n",
        "    fishing_and_geared=0\n",
        "    not_fishing_and_geared=0\n",
        "    not_fishing_and_not_geared=0\n",
        "    this_frame = dict_log[i]\n",
        "    for j in range(len(this_frame)):\n",
        "      if 'Fishing' in this_frame[j]:\n",
        "        fishing+=1\n",
        "        if 'Not Geared' in this_frame[j]:\n",
        "          fishing_and_not_geared+=1\n",
        "          not_geared+=1\n",
        "        else:\n",
        "          fishing_and_geared+=1\n",
        "          geared+=1\n",
        "      else:\n",
        "        not_fishing+=1\n",
        "        if 'Not Geared' in this_frame[j]:\n",
        "          not_fishing_and_not_geared+=1\n",
        "          not_geared+=1\n",
        "        else:\n",
        "          not_fishing_and_geared+=1\n",
        "          geared+=1\n",
        "    dumb_dict['fishing'].append(fishing)\n",
        "    dumb_dict['not_fishing'].append(not_fishing)\n",
        "    dumb_dict['geared'].append(geared)\n",
        "    dumb_dict['not_geared'].append(not_geared)\n",
        "    dumb_dict['fishing_and_not_geared'].append(fishing_and_not_geared)\n",
        "    dumb_dict['fishing_and_geared'].append(fishing_and_geared)\n",
        "    dumb_dict['not_fishing_and_geared'].append(not_fishing_and_geared)\n",
        "    dumb_dict['not_fishing_and_not_geared'].append(not_fishing_and_not_geared)\n",
        "  return dumb_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r5FNuiRPWKMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Object detection imports\n",
        "Here are the imports from the object detection module."
      ]
    },
    {
      "metadata": {
        "id": "bm0_uNRnWKMN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FishingDetection:\n",
        "    \n",
        "    frame = 0\n",
        "    jlog={}\n",
        "    def __init__(self, human_detection_graph_path,\n",
        "                 human_detection_graph_label_path,\n",
        "                 num_classes,\n",
        "                 fishing_classification_graph_path,\n",
        "                 fishing_classification_graph_label_path,\n",
        "                 geared_classification_graph_path):\n",
        "        self.__human_detection_graph_path = human_detection_graph_path\n",
        "        self.__human_detection_graph = FishingDetection.load_graph(self.__human_detection_graph_path)\n",
        "        self.__num_classes = num_classes\n",
        "        self.__fishing_classification_graph_path = fishing_classification_graph_path\n",
        "        self.__fishing_classification_graph = FishingDetection.load_inception(self.__fishing_classification_graph_path)\n",
        "        self.__label_map = label_map_util.load_labelmap(human_detection_graph_label_path)\n",
        "        self.__categories = label_map_util.convert_label_map_to_categories(self.__label_map,\n",
        "                                                                           max_num_classes=self.__num_classes,\n",
        "                                                                           use_display_name=True)\n",
        "        self.__category_index = label_map_util.create_category_index(self.__categories)\n",
        "\n",
        "        self.__geared_classification_graph_path = geared_classification_graph_path\n",
        "        self.__geared_classification_graph = FishingDetection.load_inception(self.__geared_classification_graph_path)\n",
        "        self.__fishing_session = tf.Session(graph=self.__fishing_classification_graph) \n",
        "        self.__geared_session = tf.Session(graph = self.__geared_classification_graph)\n",
        "        self.__human_detection_session = tf.Session(graph = self.__human_detection_graph)\n",
        "\n",
        "    @staticmethod\n",
        "    def nvidia_net(drop_prob = 0.2):\n",
        "      #create a sequential Model\n",
        "      model = Sequential()\n",
        "\n",
        "      #Add a Cropping layer to trim the unneeded portions of the IMAGE from the feed\n",
        "      #model.add(Cropping2D)\n",
        "      #model.add(Reshape((50,50,3), input_shape=(None,None,3))\n",
        "      model.add(Cropping2D(cropping = ((0, 0), (0,0)), input_shape = (100 ,100 ,3)))\n",
        "\n",
        "      #Normalization Layer\n",
        "      #model.add(Lambda(lambda X_input: (X_input/255.0 - 0.5)))\n",
        "\n",
        "      #Conv2D Layer 1 with 5 x 5 kernal size\n",
        "      #model.add(Convolution2D(nb_filter = 3, nb_row = 5, nb_col = 5))\n",
        "      model.add(Convolution2D(nb_filter = 12, nb_row = 3, nb_col = 3, subsample=(1,1)))\n",
        "      model.add(Activation('relu'))\n",
        "\n",
        "      #Dropout layer\n",
        "      model.add(Dropout(drop_prob))\n",
        "\n",
        "      #Conv2D Layer 2 with 5 x 5 kernal size\n",
        "      #model.add(Convolution2D(nb_filter = 24, nb_row = 5, nb_col = 5))\n",
        "      model.add(Conv2D(nb_filter = 24, nb_row = 3, nb_col = 3,subsample=(2,2)))\n",
        "      model.add(Activation('relu'))\n",
        "\n",
        "      #Conv2D Layer 3 with 5 x 5 kernal size\n",
        "      #model.add(Convolution2D(nb_filter = 36, nb_row = 5, nb_col =  5))\n",
        "      model.add(Conv2D(nb_filter = 36, nb_row = 3, nb_col =  3,subsample=(1,1)))\n",
        "      model.add(Activation('relu'))\n",
        "\n",
        "      #Dropout layer\n",
        "      model.add(Dropout(drop_prob))\n",
        "\n",
        "      #Conv2D Layer 4 with 3 x 3 kernal size\n",
        "      #model.add(Convolution2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
        "      model.add(Conv2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
        "      model.add(Activation('relu'))\n",
        "\n",
        "      #Conv2D Layer 5 with 3 x 3 kernal size\n",
        "      #model.add(Convolution2D(nb_filter = 48, nb_row = 3, nb_col = 3))\n",
        "      model.add(Conv2D(nb_filter = 64, nb_row = 3, nb_col = 3))\n",
        "      model.add(Activation('relu'))\n",
        "\n",
        "      #flatten layer\n",
        "      #flatten the output from convolution Layer\n",
        "      model.add(Flatten())\n",
        "\n",
        "      #Fully connected layer 1\n",
        "      model.add(Dense(output_dim = 50))\n",
        "\n",
        "      #Fully connected Layer 2\n",
        "      model.add(Dense(output_dim = 25))\n",
        "\n",
        "      #Fully connected Layer 3\n",
        "      model.add(Dense(output_dim = 10))\n",
        "\n",
        "      #output Layers\n",
        "      model.add(Dense(output_dim = 2, activation='softmax') )\n",
        "\n",
        "      return model\n",
        "\n",
        "    @staticmethod\n",
        "    def load_graph(graph_path):\n",
        "        detection_graph = tf.Graph()\n",
        "        with detection_graph.as_default():\n",
        "            od_graph_def = tf.GraphDef()\n",
        "            with tf.gfile.GFile(graph_path, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "        return detection_graph\n",
        "\n",
        "    @staticmethod\n",
        "    def load_inception(graph_path):\n",
        "        graph = tf.Graph()\n",
        "        graph_def = tf.GraphDef()\n",
        "        with open(graph_path, \"rb\") as f:\n",
        "            graph_def.ParseFromString(f.read())\n",
        "        with graph.as_default():\n",
        "            tf.import_graph_def(graph_def)\n",
        "        return graph\n",
        "\n",
        "    def load_image_into_numpy_array(self, image):\n",
        "        (im_width, im_height) = image.size\n",
        "        return np.array(image.getdata()).reshape(\n",
        "            (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "    def run_inference_for_single_image(self,\n",
        "                                       image,\n",
        "                                       graph,\n",
        "                                       sess):\n",
        "        with graph.as_default():\n",
        "              # Get handles to input and output tensors\n",
        "              ops = tf.get_default_graph().get_operations()\n",
        "              all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "              tensor_dict = {}\n",
        "              for key in [\n",
        "                  'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                  'detection_classes', 'detection_masks'\n",
        "              ]:\n",
        "                  tensor_name = key + ':0'\n",
        "                  if tensor_name in all_tensor_names:\n",
        "                      tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                          tensor_name)\n",
        "              if 'detection_masks' in tensor_dict:\n",
        "                  # The following processing is only for single image\n",
        "                  detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "                  detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "                  # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                  real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "                  detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "                  detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "                  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                      detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                  detection_masks_reframed = tf.cast(\n",
        "                      tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                  # Follow the convention by adding back the batch dimension\n",
        "                  tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                      detection_masks_reframed, 0)\n",
        "              image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "              # Run inference\n",
        "              output_dict = sess.run(tensor_dict,\n",
        "                                     feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "              # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "              output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "              output_dict['detection_classes'] = output_dict[\n",
        "                  'detection_classes'][0].astype(np.uint8)\n",
        "              output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "              output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "              if 'detection_masks' in output_dict:\n",
        "                  output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "        return output_dict\n",
        "\n",
        "    def list_files(self, path):\n",
        "        # files = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "        # return files\n",
        "        return glob.glob(path)\n",
        "\n",
        "    def process_files(self,\n",
        "                      list_files,\n",
        "                      label):\n",
        "        output = []\n",
        "        for i in list_files:\n",
        "            temp = []\n",
        "            temp.append(i)\n",
        "            ground_truth = [0, 0]\n",
        "            ground_truth[label] = 1.\n",
        "            temp.append(ground_truth)\n",
        "            output.append(temp)\n",
        "        return output\n",
        "\n",
        "    def read_image(self, path):\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = self.make_square(img)\n",
        "        img = img.resize((100, 100), Image.ANTIALIAS)\n",
        "        return np.asarray(img)\n",
        "\n",
        "    def pred_keras(self,\n",
        "                   img):\n",
        "        # print(img)\n",
        "        img = self.make_square(img)\n",
        "        img = img.resize((100, 100), Image.ANTIALIAS)\n",
        "        img = np.asarray(img)\n",
        "        img = (img/255)-0.5\n",
        "        return self.__gear_model.predict(np.expand_dims(img, axis=0))\n",
        "\n",
        "    def make_square(self,\n",
        "                    im,\n",
        "                    min_size=100,\n",
        "                    fill_color=(0, 0, 0, 0)):\n",
        "        im = Image.fromarray(im, 'RGB')\n",
        "        x, y = im.size\n",
        "        size = max(min_size, x, y)\n",
        "        new_im = Image.new('RGB', (size, size), fill_color)\n",
        "        # print((int((size - x) / 2), int(size - y) / 2))\n",
        "        new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
        "        return new_im\n",
        "\n",
        "    @staticmethod\n",
        "    def load_trained_model_keras(weights_path):\n",
        "        model = FishingDetection.nvidia_net()\n",
        "        model.load_weights(weights_path)\n",
        "        return model\n",
        "\n",
        "    def load_labels(self, label_file):\n",
        "        label = []\n",
        "        proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\n",
        "        for l in proto_as_ascii_lines:\n",
        "            label.append(l.rstrip())\n",
        "        return label\n",
        "\n",
        "    def read_tensor_from_image_file(self,\n",
        "                                    image_np,\n",
        "                                    input_height=299,\n",
        "                                    input_width=299,\n",
        "                                    input_mean=0,\n",
        "                                    input_std=255):\n",
        "        input_name = \"file_reader\"\n",
        "        output_name = \"normalized\"\n",
        "        file_reader = image_np\n",
        "        image_reader = tf.convert_to_tensor(image_np, np.uint8)\n",
        "        float_caster = tf.cast(image_reader, tf.float32)\n",
        "        dims_expander = tf.expand_dims(float_caster, 0)\n",
        "        resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\n",
        "        normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n",
        "        sess = tf.Session()\n",
        "        result = sess.run(normalized)\n",
        "        return result\n",
        "\n",
        "    def pred_tensorflow(self,\n",
        "                        image_np,\n",
        "                        graph,\n",
        "                        sess,\n",
        "                        input_height=299,\n",
        "                        input_width=299,\n",
        "                        input_mean=0,\n",
        "                        input_std=255,\n",
        "                        ):\n",
        "        #sess = tf.Session(graph=graph)\n",
        "        # label_file = r'D:\\fishing_detection\\data\\output_labels.txt'\n",
        "        t = self.read_tensor_from_image_file(\n",
        "            image_np,\n",
        "            input_height=input_height,\n",
        "            input_width=input_width,\n",
        "            input_mean=input_mean,\n",
        "            input_std=input_std)\n",
        "        input_name = \"import/\" + \"Placeholder\"\n",
        "        output_name = \"import/\" + \"final_result\"\n",
        "        input_operation = graph.get_operation_by_name(input_name)\n",
        "        output_operation = graph.get_operation_by_name(output_name)\n",
        "        results = sess.run(output_operation.outputs[0], {\n",
        "            input_operation.outputs[0]: t\n",
        "        })\n",
        "        results = np.squeeze(results)\n",
        "        return results\n",
        "\n",
        "    def predict_fishing(self, image, boxes, classes, scores):\n",
        "        im_width, im_height = image.size\n",
        "        image_np = self.load_image_into_numpy_array(image)\n",
        "        predictions = {}\n",
        "        predictions['bo'] = []\n",
        "        predictions['res'] = []\n",
        "        predictions['score'] = []\n",
        "        predictions1 = {}\n",
        "        predictions1['bo'] = []\n",
        "        predictions1['res'] = []\n",
        "        predictions1['score'] = []\n",
        "        for i in range(len(boxes)):\n",
        "            if classes[i] == 1 and scores[i] > 0.3:\n",
        "                predictions['bo'].append(boxes[i])\n",
        "                predictions1['bo'].append(boxes[i])\n",
        "                box = tuple(boxes[i].tolist())\n",
        "                ymin, xmin, ymax, xmax = box\n",
        "                ymin = int(ymin * im_height)\n",
        "                xmin = int(xmin * im_width)\n",
        "                ymax = int(ymax * im_height)\n",
        "                xmax = int(xmax * im_width)\n",
        "                roi = image_np[ymin:ymax, xmin:xmax]\n",
        "                # c = pred(cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\n",
        "                c = self.pred_tensorflow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB), self.__fishing_classification_graph, self.__fishing_session)\n",
        "                d = self.pred_tensorflow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB), self.__geared_classification_graph ,self.__geared_session)\n",
        "                if (c[0] > 0.5):\n",
        "                    predictions['res'].append(0)\n",
        "                    predictions['score'].append(c[0])\n",
        "                else:\n",
        "                    predictions['res'].append(1)\n",
        "                    predictions['score'].append(c[1])\n",
        "                if (d[0] > 0.5):\n",
        "                    predictions1['res'].append(0)\n",
        "                    predictions1['score'].append(d[0])\n",
        "                else:\n",
        "                    predictions1['res'].append(1)\n",
        "                    predictions1['score'].append(d[1])    \n",
        "        return predictions,predictions1\n",
        "\n",
        "    def pipeline(self, img):\n",
        "        FishingDetection.frame +=1\n",
        "        image = Image.fromarray(img)\n",
        "        image_np = self.load_image_into_numpy_array(image)\n",
        "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "        output_dict = self.run_inference_for_single_image(image_np, self.__human_detection_graph, self.__human_detection_session)\n",
        "        for i in range(len(output_dict['detection_classes'])):\n",
        "            if output_dict['detection_classes'][i] != 1:\n",
        "                output_dict['detection_scores'][i] = 0.0\n",
        "        predi,predi1 = self.predict_fishing(image, output_dict['detection_boxes'], output_dict['detection_classes'],\n",
        "                                     output_dict['detection_scores'])\n",
        "        category_ind1 = {0: {'id': 0, 'name': 'Geared'}, 1: {'id': 1, 'name': 'Not Geared'}}\n",
        "        category_ind = {1: {'id': 1, 'name': 'Not Fishing'}, 0: {'id': 0, 'name': 'Fishing'}}\n",
        "        \n",
        "        frame_log = make_frame_log(np.asarray(predi['bo'], dtype=np.float32),\n",
        "            predi['res'],predi1['res'],\n",
        "            category_ind,category_ind1,)\n",
        "        \n",
        "        FishingDetection.jlog[FishingDetection.frame]=frame_log\n",
        "        \n",
        "        visualize_boxes_and_labels_on_image_array(\n",
        "            image_np,\n",
        "            np.asarray(predi['bo'], dtype=np.float32),\n",
        "            predi['res'],predi1['res'],\n",
        "            predi['score'],predi1['score'],\n",
        "            category_ind,category_ind1,\n",
        "            instance_masks=output_dict.get('detection_masks'),\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=4)\n",
        "        return image_np\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wdyc3gIt6Xze",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    INPUT_DIRECTORY = 'input_video'\n",
        "    OUTPUT_DIRECTORY = 'output_video'\n",
        "    INPUT_FILE = 'people_fishing.mp4'\n",
        "    OUTPUT_FILE = 'people_fishing_out1.mp4'\n",
        "    vid_output = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILE)\n",
        "    vid_input = os.path.join(INPUT_DIRECTORY, INPUT_FILE)\n",
        "    subclip_start = '00:04:05.00'\n",
        "    subclip_end = '00:04:15.00'\n",
        "    clip = VideoFileClip(vid_input).subclip(subclip_start, subclip_end)\n",
        "\n",
        "    DIRECTORY_NAME = 'models'\n",
        "    MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
        "    TAR_EXTENSION = '.tar.gz'\n",
        "    MODEL_FILE = MODEL_NAME + TAR_EXTENSION\n",
        "    GRAPH_NAME = 'frozen_inference_graph.pb'\n",
        "    DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "    PATH_TO_FROZEN_GRAPH = os.path.join(DIRECTORY_NAME, MODEL_NAME, GRAPH_NAME)\n",
        "\n",
        "    # List of the strings that is used to add correct label for each box.\n",
        "    PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
        "\n",
        "    NUM_CLASSES = 90\n",
        "    fishing_detection_graph = os.path.join('colab', 'fishing_model', 'output_graph.pb')\n",
        "    geared_detection_graph = os.path.join('colab', 'gear_detect_models', 'output_graph.pb')\n",
        "    fishing_classification_graph_label_path = os.path.join('data', 'output_labels.txt')\n",
        "    object_detect = FishingDetection(human_detection_graph_path=PATH_TO_FROZEN_GRAPH,\n",
        "                                     human_detection_graph_label_path=PATH_TO_LABELS,\n",
        "                                     num_classes=NUM_CLASSES,\n",
        "                                     fishing_classification_graph_path=fishing_detection_graph,\n",
        "                                     fishing_classification_graph_label_path=fishing_classification_graph_label_path,\n",
        "                                     geared_classification_graph_path=geared_detection_graph)\n",
        "    vid = clip.fl_image(object_detect.pipeline)\n",
        "    vid.write_videofile(vid_output, audio=False)\n",
        "    with open('data.json', 'w') as fp:\n",
        "      json.dump(FishingDetection.jlog, fp)\n",
        "    new_log = make_dummer_log(FishingDetection.jlog)\n",
        "    new_log['length'] = vid.duration\n",
        "    with open('data_disp.json', 'w') as fp:\n",
        "      json.dump(new_log, fp)\n",
        "    \n",
        "    #Reset class variables to default values  \n",
        "    FishingDetection.jlog={}\n",
        "    FishingDetection.frame=0\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3znRPqDz6j2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1836
        },
        "outputId": "08021095-ac71-468d-ef52-80d2b0af8167"
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] >>>> Building video output_video/people_fishing_out1.mp4\n",
            "[MoviePy] Writing video output_video/people_fishing_out1.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/101 [00:10<16:47, 10.08s/it]\u001b[A\n",
            "  2%|▏         | 2/101 [00:22<18:33, 11.25s/it]\u001b[A\n",
            "  3%|▎         | 3/101 [00:39<21:21, 13.07s/it]\u001b[A\n",
            "  4%|▍         | 4/101 [00:53<21:33, 13.33s/it]\u001b[A\n",
            "  5%|▍         | 5/101 [01:00<19:13, 12.01s/it]\u001b[A\n",
            "  6%|▌         | 6/101 [01:12<19:07, 12.08s/it]\u001b[A\n",
            "  7%|▋         | 7/101 [01:23<18:36, 11.87s/it]\u001b[A\n",
            "  8%|▊         | 8/101 [01:33<18:07, 11.69s/it]\u001b[A\n",
            "  9%|▉         | 9/101 [01:41<17:20, 11.31s/it]\u001b[A\n",
            " 10%|▉         | 10/101 [01:48<16:24, 10.82s/it]\u001b[A\n",
            " 11%|█         | 11/101 [01:58<16:08, 10.76s/it]\u001b[A\n",
            " 12%|█▏        | 12/101 [02:06<15:41, 10.57s/it]\u001b[A\n",
            " 13%|█▎        | 13/101 [02:15<15:15, 10.40s/it]\u001b[A\n",
            " 14%|█▍        | 14/101 [02:28<15:25, 10.64s/it]\u001b[A\n",
            " 15%|█▍        | 15/101 [02:39<15:13, 10.62s/it]\u001b[A\n",
            " 16%|█▌        | 16/101 [02:52<15:14, 10.75s/it]\u001b[A\n",
            " 17%|█▋        | 17/101 [03:04<15:09, 10.83s/it]\u001b[A\n",
            " 18%|█▊        | 18/101 [03:16<15:06, 10.92s/it]\u001b[A\n",
            " 19%|█▉        | 19/101 [03:26<14:52, 10.89s/it]\u001b[A\n",
            " 20%|█▉        | 20/101 [03:39<14:47, 10.95s/it]\u001b[A\n",
            " 21%|██        | 21/101 [03:47<14:26, 10.84s/it]\u001b[A\n",
            " 22%|██▏       | 22/101 [03:58<14:14, 10.82s/it]\u001b[A\n",
            " 23%|██▎       | 23/101 [04:10<14:09, 10.89s/it]\u001b[A\n",
            " 24%|██▍       | 24/101 [04:25<14:10, 11.05s/it]\u001b[A\n",
            " 25%|██▍       | 25/101 [04:39<14:09, 11.18s/it]\u001b[A\n",
            " 26%|██▌       | 26/101 [04:49<13:56, 11.15s/it]\u001b[A\n",
            " 27%|██▋       | 27/101 [04:58<13:38, 11.07s/it]\u001b[A\n",
            " 28%|██▊       | 28/101 [05:12<13:35, 11.17s/it]\u001b[A\n",
            " 29%|██▊       | 29/101 [05:27<13:33, 11.30s/it]\u001b[A\n",
            " 30%|██▉       | 30/101 [05:38<13:20, 11.28s/it]\u001b[A\n",
            " 31%|███       | 31/101 [05:47<13:04, 11.21s/it]\u001b[A\n",
            " 32%|███▏      | 32/101 [06:04<13:06, 11.40s/it]\u001b[A\n",
            " 33%|███▎      | 33/101 [06:17<12:58, 11.45s/it]\u001b[A\n",
            " 34%|███▎      | 34/101 [06:26<12:42, 11.38s/it]\u001b[A\n",
            " 35%|███▍      | 35/101 [06:38<12:30, 11.37s/it]\u001b[A\n",
            " 36%|███▌      | 36/101 [06:46<12:14, 11.31s/it]\u001b[A\n",
            " 37%|███▋      | 37/101 [07:00<12:06, 11.35s/it]\u001b[A\n",
            " 38%|███▊      | 38/101 [07:07<11:47, 11.24s/it]\u001b[A\n",
            " 39%|███▊      | 39/101 [07:14<11:30, 11.13s/it]\u001b[A\n",
            " 40%|███▉      | 40/101 [07:23<11:15, 11.08s/it]\u001b[A\n",
            " 41%|████      | 41/101 [07:38<11:10, 11.18s/it]\u001b[A\n",
            " 42%|████▏     | 42/101 [07:53<11:05, 11.27s/it]\u001b[A\n",
            " 43%|████▎     | 43/101 [08:06<10:56, 11.32s/it]\u001b[A\n",
            " 44%|████▎     | 44/101 [08:22<10:50, 11.41s/it]\u001b[A\n",
            " 45%|████▍     | 45/101 [08:33<10:38, 11.41s/it]\u001b[A\n",
            " 46%|████▌     | 46/101 [08:44<10:27, 11.41s/it]\u001b[A\n",
            " 47%|████▋     | 47/101 [09:00<10:21, 11.50s/it]\u001b[A\n",
            " 48%|████▊     | 48/101 [09:16<10:14, 11.59s/it]\u001b[A\n",
            " 49%|████▊     | 49/101 [09:34<10:09, 11.72s/it]\u001b[A\n",
            " 50%|████▉     | 50/101 [09:49<10:01, 11.80s/it]\u001b[A\n",
            " 50%|█████     | 51/101 [10:05<09:53, 11.88s/it]\u001b[A\n",
            " 51%|█████▏    | 52/101 [10:23<09:47, 11.99s/it]\u001b[A\n",
            " 52%|█████▏    | 53/101 [10:43<09:43, 12.15s/it]\u001b[A\n",
            " 53%|█████▎    | 54/101 [11:02<09:36, 12.26s/it]\u001b[A\n",
            " 54%|█████▍    | 55/101 [11:18<09:27, 12.33s/it]\u001b[A\n",
            " 55%|█████▌    | 56/101 [11:34<09:18, 12.40s/it]\u001b[A\n",
            " 56%|█████▋    | 57/101 [11:52<09:10, 12.51s/it]\u001b[A\n",
            " 57%|█████▋    | 58/101 [12:07<08:59, 12.54s/it]\u001b[A\n",
            " 58%|█████▊    | 59/101 [12:23<08:49, 12.61s/it]\u001b[A\n",
            " 59%|█████▉    | 60/101 [12:42<08:41, 12.71s/it]\u001b[A\n",
            " 60%|██████    | 61/101 [12:59<08:30, 12.77s/it]\u001b[A\n",
            " 61%|██████▏   | 62/101 [13:13<08:19, 12.80s/it]\u001b[A\n",
            " 62%|██████▏   | 63/101 [13:27<08:07, 12.83s/it]\u001b[A\n",
            " 63%|██████▎   | 64/101 [13:42<07:55, 12.85s/it]\u001b[A\n",
            " 64%|██████▍   | 65/101 [13:59<07:44, 12.91s/it]\u001b[A\n",
            " 65%|██████▌   | 66/101 [14:14<07:32, 12.94s/it]\u001b[A\n",
            " 66%|██████▋   | 67/101 [14:29<07:21, 12.97s/it]\u001b[A\n",
            " 67%|██████▋   | 68/101 [14:46<07:10, 13.03s/it]\u001b[A\n",
            " 68%|██████▊   | 69/101 [15:01<06:57, 13.06s/it]\u001b[A\n",
            " 69%|██████▉   | 70/101 [15:11<06:43, 13.02s/it]\u001b[A\n",
            " 70%|███████   | 71/101 [15:24<06:30, 13.01s/it]\u001b[A\n",
            " 71%|███████▏  | 72/101 [15:41<06:19, 13.08s/it]\u001b[A\n",
            " 72%|███████▏  | 73/101 [15:53<06:05, 13.07s/it]\u001b[A\n",
            " 73%|███████▎  | 74/101 [16:11<05:54, 13.13s/it]\u001b[A\n",
            " 74%|███████▍  | 75/101 [16:22<05:40, 13.09s/it]\u001b[A\n",
            " 75%|███████▌  | 76/101 [16:32<05:26, 13.06s/it]\u001b[A\n",
            " 76%|███████▌  | 77/101 [16:43<05:12, 13.03s/it]\u001b[A\n",
            " 77%|███████▋  | 78/101 [16:54<04:59, 13.01s/it]\u001b[A\n",
            " 78%|███████▊  | 79/101 [17:07<04:46, 13.01s/it]\u001b[A\n",
            " 79%|███████▉  | 80/101 [17:20<04:33, 13.01s/it]\u001b[A\n",
            " 80%|████████  | 81/101 [17:36<04:20, 13.04s/it]\u001b[A\n",
            " 81%|████████  | 82/101 [17:44<04:06, 12.98s/it]\u001b[A\n",
            " 82%|████████▏ | 83/101 [17:57<03:53, 12.99s/it]\u001b[A\n",
            " 83%|████████▎ | 84/101 [18:15<03:41, 13.05s/it]\u001b[A\n",
            " 84%|████████▍ | 85/101 [18:31<03:29, 13.08s/it]\u001b[A\n",
            " 85%|████████▌ | 86/101 [18:42<03:15, 13.05s/it]\u001b[A\n",
            " 86%|████████▌ | 87/101 [18:53<03:02, 13.02s/it]\u001b[A\n",
            " 87%|████████▋ | 88/101 [19:06<02:49, 13.03s/it]\u001b[A\n",
            " 88%|████████▊ | 89/101 [19:17<02:36, 13.00s/it]\u001b[A\n",
            " 89%|████████▉ | 90/101 [19:30<02:23, 13.01s/it]\u001b[A\n",
            " 90%|█████████ | 91/101 [19:46<02:10, 13.04s/it]\u001b[A\n",
            " 91%|█████████ | 92/101 [19:57<01:57, 13.01s/it]\u001b[A\n",
            " 92%|█████████▏| 93/101 [20:05<01:43, 12.97s/it]\u001b[A\n",
            " 93%|█████████▎| 94/101 [20:16<01:30, 12.94s/it]\u001b[A\n",
            " 94%|█████████▍| 95/101 [20:27<01:17, 12.92s/it]\u001b[A\n",
            " 95%|█████████▌| 96/101 [20:28<01:03, 12.80s/it]\u001b[A\n",
            " 96%|█████████▌| 97/101 [20:29<00:50, 12.68s/it]\u001b[A\n",
            " 97%|█████████▋| 98/101 [20:30<00:37, 12.56s/it]\u001b[A\n",
            " 98%|█████████▊| 99/101 [20:31<00:24, 12.44s/it]\u001b[A\n",
            " 99%|█████████▉| 100/101 [20:33<00:12, 12.33s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] >>>> Video ready: output_video/people_fishing_out1.mp4 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "261FmZxFAgWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4054960a-256d-49e5-a6ad-f7ae3931e9a9"
      },
      "cell_type": "code",
      "source": [
        "INPUT_DIRECTORY = 'input_video'\n",
        "OUTPUT_DIRECTORY = 'output_video'\n",
        "INPUT_FILE = 'people_fishing.mp4'\n",
        "OUTPUT_FILE = 'people_fishing_in.mp4'\n",
        "vid_output = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILE)\n",
        "vid_input = os.path.join(INPUT_DIRECTORY, INPUT_FILE)\n",
        "subclip_start = '00:04:05.00'\n",
        "subclip_end = '00:04:15.00'\n",
        "clip = VideoFileClip(vid_input).subclip(subclip_start, subclip_end)\n",
        "vid =clip\n",
        "vid.write_videofile(vid_output, audio=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] >>>> Building video output_video/people_fishing_in.mp4\n",
            "[MoviePy] Writing video output_video/people_fishing_in.mp4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\n",
            " 19%|█▉        | 19/101 [00:00<00:00, 172.74it/s]\u001b[A\n",
            " 36%|███▌      | 36/101 [00:00<00:00, 167.48it/s]\u001b[A\n",
            " 47%|████▋     | 47/101 [00:00<00:00, 115.28it/s]\u001b[A\n",
            " 54%|█████▍    | 55/101 [00:00<00:00, 84.92it/s] \u001b[A\n",
            " 61%|██████▏   | 62/101 [00:00<00:00, 73.55it/s]\u001b[A\n",
            " 67%|██████▋   | 68/101 [00:01<00:00, 66.10it/s]\u001b[A\n",
            " 72%|███████▏  | 73/101 [00:01<00:00, 61.71it/s]\u001b[A\n",
            " 77%|███████▋  | 78/101 [00:01<00:00, 58.54it/s]\u001b[A\n",
            " 82%|████████▏ | 83/101 [00:01<00:00, 55.37it/s]\u001b[A\n",
            " 86%|████████▌ | 87/101 [00:01<00:00, 53.60it/s]\u001b[A\n",
            " 90%|█████████ | 91/101 [00:01<00:00, 51.59it/s]\u001b[A\n",
            " 94%|█████████▍| 95/101 [00:01<00:00, 48.46it/s]\u001b[A\n",
            " 97%|█████████▋| 98/101 [00:02<00:00, 46.16it/s]\u001b[A\n",
            " 99%|█████████▉| 100/101 [00:02<00:00, 44.62it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[MoviePy] Done.\n",
            "[MoviePy] >>>> Video ready: output_video/people_fishing_in.mp4 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "utyZfEoCJ5iN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('data_disp.json') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}